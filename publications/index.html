<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Mathieu Blondel // Publications</title>
<link rel="stylesheet" href="../style.css" type="text/css" media="screen">
</head>

<body>

<div id="topbar">
  <div id="header">
    <h1>Mathieu Blondel</h1>
  </div>
</div>

<div id="navigation">
  <p>
  <a href="../">Home</a> /
  <a href="." class="active">Publications</a> /
  <a href="../talks/">Talks</a> /
  <a href="../teaching/">Teaching</a> /
  <a href="../students/">Students</a> /
  <a href="../code/">Software</a> /
  <a href="../data/">Data</a> 
  </p>
</div>

<div id="main">

<p>For a full list, see my Google scholar <a href="http://scholar.google.com/citations?user=C0EKzrUAAAAJ">profile</a>.</p>

<h2>Research topics</h2>

<ul>
    <li><a href="#dp">Differentiable programming</a></li>
    <li><a href="#lm">Language models</a></li>
    <li><a href="#dl">Deep learning</a></li>
    <li><a href="#lf">Loss functions</a></li>
    <li><a href="#optim">Optimization</a></li>
    <li><a href="#ot">Optimal transport</a></li>
    <li><a href="#pn">Polynomial networks</a></li>
    <li><a href="#mls">Machine learning software</a></li>
</ul>

<h2 id="dp">Differentiable programming</h2>

<ul class="pub">

<li>
The Elements of Differentiable Programming.<br />
<u>Mathieu Blondel</u>, Vincent Roulet. <br />
Book draft, March 2024. <br />
<a href="https://diffprog.github.io">Website</a> / 
<a href="https://arxiv.org/abs/2403.14606">arXiv</a> / 
<a href="https://github.com/diffprog/code">Code</a>
</li>

<li>
Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective.<br />
Michael Sander, Joan Puigcerver, Josip Djolonga, Gabriel Peyré, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2023. <br />
<a href="https://arxiv.org/abs/2302.01425">arXiv</a> /
<a href="https://github.com/google-research/google-research/tree/master/sparse_soft_topk">Code</a> /
<a href="../talks/mblondel-ICML-2023-07.pdf">Slides</a>
</li>

<li>
Deep embedding and alignment of protein sequences.<br />
Felipe Llinares-López, Quentin Berthet, <u>Mathieu Blondel</u>,
Olivier Teboul, Jean-Philippe Vert. <br />
<strong>Nature Methods</strong>, volume 20, pages 104–111, 2023. <br />
<a href="https://www.nature.com/articles/s41592-022-01700-2">Link</a> /
<a href="https://www.biorxiv.org/content/10.1101/2021.11.15.468653">bioRxiv</a>
/ <a href="https://github.com/google-research/google-research/tree/master/dedal">Code</a>
</li>

<li>
Efficient and Modular Implicit Differentiation. <br />
<u>Mathieu Blondel</u>, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-López, Fabian Pedregosa, Jean-Philippe Vert. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2022. <br />
<a href="https://arxiv.org/abs/2105.15183">arXiv</a> /
<a href="https://github.com/google/jaxopt">Code</a>
</li>

<li>
Differentiable Divergences Between Time Series. <br />
<u>Mathieu Blondel</u>, Arthur Mensch, Jean-Philippe Vert. <br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), April 2021. <br />
<a href="https://arxiv.org/abs/2010.08354">arXiv</a> /
<a href="https://github.com/google-research/soft-dtw-divergences">Code</a>
</li>

<li>
Learning with Differentiable Perturbed Optimizers. <br />
Quentin Berthet, <u>Mathieu Blondel</u>, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2020. <br />
<a href="https://arxiv.org/abs/2002.08676">arXiv</a> / 
<a href="https://github.com/google-research/google-research/tree/master/perturbations">Code</a>
</li>

<li>
Fast Differentiable Sorting and Ranking. <br />
<u>Mathieu Blondel</u>, Olivier Teboul, Quentin Berthet, Josip Djolonga. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2020. <br />
<a href="https://arxiv.org/abs/2002.08871">arXiv</a> /
<a href="https://github.com/google-research/fast-soft-sort">Code</a> (see also PyTorch <a href="https://github.com/teddykoker/torchsort">reimplementation</a> by Teddy Koker).
</li>

<li>
SparseMAP: Differentiable Sparse Structured Inference. <br />
Vlad Niculae, André F. T. Martins, <u>Mathieu Blondel</u>, Claire Cardie. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2018. <br />
<a href="https://arxiv.org/abs/1802.04223">arXiv</a> / 
<a href="https://github.com/vene/sparsemap">Code</a>
</li>

<li>
Differentiable Dynamic Programming for Structured Prediction and Attention. <br />
Arthur Mensch, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2018. <br />
<a href="https://arxiv.org/abs/1802.03676">arXiv</a> /
<a href="https://github.com/arthurmensch/didyprog">Code</a>
</li>


<li>
A Regularized Framework for Sparse and Structured Neural Attention. <br />
Vlad Niculae, <u>Mathieu Blondel</u>.<br />
In Proceedings of Neural Information Processing Systems (<strong>NIPS</strong>), December 2017. <br />
<a href="https://arxiv.org/abs/1705.07704">arXiv</a> / 
<a href="https://github.com/vene/sparse-structured-attention">Code</a>
</li>


<li>
Soft-DTW: a Differentiable Loss Function for Time-Series. <br />
Marco Cuturi, <u>Mathieu Blondel</u>.<br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), August 2017. <br />
<a href="mcuturi-mblondel-icml2017.pdf">PDF</a> / 
<a href="https://github.com/mblondel/soft-dtw">Code</a>
</li>

</ul>

<h2 id="lm">Language models</h2>

<ul class="pub">

<li>
On Teacher Hacking in Language Model Distillation. <br />
Daniil Tiapkin, Daniele Calandriello, Johan Ferret, Sarah Perrin, Nino Vieillard, Alexandre Ramé, <u>Mathieu Blondel</u>. <br />
Preprint. <br />
<a href="https://arxiv.org/abs/2502.02671">arXiv</a>
</li>

<li>
Direct Language Model Alignment from Online AI Feedback. <br />
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, Johan Ferret, <u>Mathieu Blondel</u>. <br />
<a href="https://arxiv.org/abs/2402.04792">arXiv</a>

</li>

<li>
Decoding-time Realignment of Language Models. <br />
Tianlin Liu, Shangmin Guo, Leonardo Bianco, Daniele Calandriello, Quentin
Berthet, Felipe Llinares, Jessica Hoffmann, Lucas Dixon, Michal Valko, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2024. 
Spotlight (3.5% acceptance rate). <br />
<a href="https://arxiv.org/abs/2402.02992">arXiv</a>
</li>
</ul>

<h2 id="dl">Deep learning</h2>

<ul class="pub">

<li>
How do Transformers perform In-Context Autoregressive Learning? <br />
Michael Sander, Raja Giryes, Taiji Suzuki, <u>Mathieu Blondel</u>, Gabriel Peyré.<br />
<a href="https://arxiv.org/abs/2402.05787">arXiv</a>
</li>

<li>
Routers in Vision Mixture of Experts: An Empirical Study.<br />
Tianlin Liu, <u>Mathieu Blondel</u>, Carlos Riquelme, Joan Puigcerver.<br />
<a href="https://arxiv.org/abs/2401.15969">arxiv</a>
</li>

<li>
Sinkformers: Transformers with Doubly Stochastic Attention.<br />
Michaël Sander, Pierre Ablin, <u>Mathieu Blondel</u>, Gabriel Peyré. <br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), March 2022. <br />
<a href="https://arxiv.org/abs/2110.11773">arXiv</a> / 
<a href="https://github.com/michaelsdr/sinkformers">Code</a>
</li>

<li>
Momentum Residual Neural Networks. <br />
Michaël Sander, Pierre Ablin, <u>Mathieu Blondel</u>, Gabriel Peyré. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2021. <br />
<a href="https://arxiv.org/abs/2102.07870">arXiv</a> /
<a href="https://github.com/michaelsdr/momentumnet">Code</a>
</li>

</ul>

<h2 id="lf">Loss functions</h2>

<ul class="pub">

<li>
Joint Learning of Energy-based Models and their Partition Function. <br />
Michael E. Sander, Vincent Roulet, Tianlin Liu, <u>Mathieu Blondel</u>. <br />
Preprint. <br />
<a href="https://arxiv.org/abs/2501.18528">arXiv</a>
</li>

<li>
Loss Functions and Operators Generated by f-Divergences. <br />
Vincent Roulet, Tianlin Liu, Nino Vieillard, Michael E. Sander, <u>Mathieu Blondel</u>. <br />
Preprint. <br />
<a href="https://arxiv.org/abs/2501.18537">arXiv</a>
<li>
Learning with Fitzpatrick Losses.<br />
Seta Rakotomandimby, Jean-Philippe Chancelier, Michel de Lara, <u>Mathieu Blondel</u>. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2024. <br />
<a href="https://arxiv.org/abs/2405.14574">arXiv</a>
</li>

<li>
Learning Energy Networks with Generalized Fenchel-Young Losses.<br />
<u>Mathieu Blondel</u>, Felipe Llinares-López, Robert Dadashi, Léonard Hussenot, Matthieu Geist. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2022. <br />
<a href="https://arxiv.org/abs/2205.09589">arXiv</a>
</li>

<li>
Sparse Continuous Distributions and Fenchel-Young Losses. <br />
André F. T. Martins, Marcos Treviso, António Farinhas, Pedro M. Q. Aguiar, Mário A. T. Figueiredo, <u>Mathieu Blondel</u>, Vlad Niculae. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>). <br />
<a href="https://arxiv.org/abs/2108.01988">arXiv</a> /
<a href="https://github.com/deep-spin/sparse_continuous_distributions">Code</a>
</li>

<li>
Structured Prediction with Projection Oracles. <br />
Mathieu Blondel.<br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2019. <br />
<a href="https://arxiv.org/abs/1910.11369">arXiv</a> / <a href="https://github.com/mblondel/projection-losses">Code</a>
</li>

<li>
Geometric Losses for Distributional Learning. <br />
Arthur Mensch, <u>Mathieu Blondel</u>, Gabriel Peyré. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), June 2019. <br />
<a href="https://arxiv.org/abs/1905.06005">arXiv</a> / 
<a href="https://github.com/arthurmensch/g-softmax">Code</a>
</li>

<li>
Learning with Fenchel-Young Losses. <br />
<u>Mathieu Blondel</u>, André F. T. Martins, Vlad Niculae. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>), Volume 21, pp. 1−69, 2020. <br />
<a href="https://arxiv.org/abs/1901.02324">arXiv</a> /
<a href="https://github.com/mblondel/fenchel-young-losses">Code</a>
</li>

<li>
Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms. <br />
<u>Mathieu Blondel</u>, André F. T. Martins, Vlad Niculae. <br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), April 2019. <br />
<a href="https://arxiv.org/abs/1805.09717">arXiv</a> /
<a href="https://github.com/mblondel/fenchel-young-losses">Code</a> 
</li>

</ul>

<h2 id="optim">Optimization</h2>

<ul class="pub">

<li>
Stepping on the Edge: Curvature Aware Learning Rate Tuners. <br />
Vincent Roulet, Atish Agarwala, Jean-Bastien Grill, Grzegorz Swirszcz, <u>Mathieu Blondel</u>, Fabian Pedregosa. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2024. <br />
<a href="https://arxiv.org/abs/2407.06183">arXiv</a>
</li>

<li>
Implicit Diffusion: Efficient Optimization through Stochastic Sampling. <br />
Pierre Marion, Anna Korba, Peter Bartlett, <u>Mathieu Blondel</u>, Valentin De Bortoli, Arnaud Doucet, Felipe Llinares-López, Courtney Paquette, Quentin Berthet.<br />
In Proceedings of the International Conference on Artifical Intelligence and Statistics (<strong>AISTATS</strong>), May 2025. <br />
<a href="https://arxiv.org/abs/2402.05468">arXiv</a>
</li>

<li>
Dual Gauss-Newton Directions for Deep Learning.<br />
Vincent Roulet, <u>Mathieu Blondel</u>. <br />
<a href="https://arxiv.org/abs/2308.08886">arXiv</a>
</li>

<li>
Implicit Differentiation for Fast Hyperparameter Selection in Non-Smooth Convex Learning. <br />
Quentin Bertrand, Quentin Klopfenstein, Mathurin Massias, <u>Mathieu Blondel</u>,
Samuel Vaiter, Alexandre Gramfort, Joseph Salmon. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>). <br />
<a href="https://arxiv.org/abs/2105.01637">arXiv</a> /
<a href="https://github.com/QB3/sparse-ho">Code</a>
</li>

<li>
Implicit Differentiation of Lasso-type Models for Hyperparameter Optimization. <br />
Quentin Bertrand, Quentin Klopfenstein, <u>Mathieu Blondel</u>, Samuel Vaiter, Alexandre Gramfort, Joseph Salmon. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2020. <br />
<a href="https://arxiv.org/abs/2002.08943">arXiv</a> /
<a href="https://github.com/QB3/sparse-ho">Code</a>
</li>

<li>
Large-scale Multiclass Support Vector Machine Training via Euclidean Projection onto the Simplex. <br />
<u>Mathieu Blondel</u>, Akinori Fujino, Naonori Ueda. <br />
In Proceedings of the 22nd International Conference on Pattern Recognition (<strong>ICPR</strong>), August 2014.<br />
<a href="mblondel-icpr2014.pdf">PDF</a> /
<a href="https://gist.github.com/mblondel/97cffbea574a5890f0d7">Code</a>
</li>

<li>
Online Passive-Aggressive Algorithms for Non-Negative Matrix Factorization and Completion. <br />
<u>Mathieu Blondel</u>, Yotaro Kubo, Naonori Ueda.  <br /> 
In Proceedings of the 17th International Conference on Artifical Intelligence and Statistics (<strong>AISTATS</strong>), April 2014. <br />
<a href="mblondel-aistats2014.pdf">PDF</a> / <a href="mblondel-aistats2014-supp.pdf">Supplementary material</a> 

<li>Block Coordinate Descent Algorithms for Large-scale Sparse Multiclass 
Classification. <br />
<u>Mathieu Blondel</u>, Kazuhiro Seki, Kuniaki Uehara. <br />
<strong>Machine Learning</strong>, May 2013. The final publication is available <a href="http://link.springer.com/article/10.1007%2Fs10994-013-5367-2">here</a>.<br/>
Open-science honorable mention at ECML PKDD 2013. <br />
<a href="mblondel-mlj2013.pdf">PDF</a> / <a href="https://github.com/scikit-learn-contrib/lightning">Code</a> / <a href="../data/">Data</a> 
</li>

</ul>

<h2 id="ot">Optimal transport</h2>

<ul class="pub">

<li>
Sparsity-Constrained Optimal Transport.<br />
Tianlin Liu, Joan Puigcerver, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Learning Representations (<strong>ICLR</strong>), May 2023. <br />
<a href="https://arxiv.org/abs/2209.15466">arXiv</a> /
<a href="https://github.com/google-research/vmoe/tree/main/vmoe/projects/sparsity_constrained_ot">Code</a>
</li>

<li>
Large-Scale Optimal Transport and Mapping Estimation. <br />
Vivien Seguy, Bharath Bhushan Damodaran, Rémi Flamary, Nicolas Courty, Antoine Rolet, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Learning Representations (<strong>ICLR</strong>), April 2018. <br />
<a href="https://arxiv.org/abs/1711.02283">arXiv</a> / 
<a href="https://github.com/vivienseguy/Large-Scale-OT">Code</a>
</li>

<li>
Smooth and Sparse Optimal Transport. <br />
<u>Mathieu Blondel</u>, Vivien Seguy, Antoine Rolet.<br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), April 2018. <br />
<a href="https://arxiv.org/abs/1710.06276">arXiv</a> / 
<a href="https://github.com/mblondel/smooth-ot">Code</a> /
<a href="https://github.com/mblondel/smooth-ot/tree/master/data">Data</a> 
</li>

</ul>

<h2 id="pn">Polynomial networks</h2>

<ul class="pub">

<li>
Multi-output Polynomial Networks and Factorization Machines. <br />
<u>Mathieu Blondel</u>, Vlad Niculae, Takuma Otsuka, Naonori Ueda.<br />
In Proceedings of Neural Information Processing Systems (<strong>NIPS</strong>), December 2017. <br />
<a href="https://arxiv.org/abs/1705.07603">arXiv</a> <br />
</li>

<li>
Higher-order Factorization Machines. <br />
<u>Mathieu Blondel</u>, Akinori Fujino, Naonori Ueda, Masakazu Ishihata. <br />
In Proceedings of Neural Information Processing Systems (<strong>NIPS</strong>), December 2016. <br />
<a href="http://arxiv.org/abs/1607.07195">arXiv</a>
</li>

<li>
Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms. <br />
<u>Mathieu Blondel</u>, Masakazu Ishihata, Akinori Fujino, Naonori Ueda. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), September 2016. <br />
<a href="mblondel-icml2016.pdf">PDF</a> <br />
Open-source <a href="https://github.com/scikit-learn-contrib/polylearn">implementation</a> by <a href="http://vene.ro/">Vlad Niculae</a>
</li>

<li>
Convex Factorization Machines. <br />
<u>Mathieu Blondel</u>, Akinori Fujino, Naonori Ueda. <br />
In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (<strong>ECML PKDD</strong>), September 2015. <br />
<a href="mblondel-ecmlpkdd2015.pdf">PDF</a> / <a href="../talks/mblondel-cambridge-2015-09.pdf">Slides</a> <br />
Open-source <a href="https://gist.github.com/vene/d0fa89c17de961a18f7b">implementation</a> by <a href="http://vene.ro/">Vlad Niculae</a>
</li>

</ul>

<h2 id="mls">Machine learning software</h2>

<ul class="pub">

<li>Scikit-learn: Machine Learning in Python. <br />
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand 
Thirion, Olivier Grisel, <u>Mathieu Blondel</u>, Peter Prettenhofer, Ron Weiss, 
Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu 
Brucher, Matthieu Perrot, Édouard Duchesnay. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>), volume 12, pp. 2825−2830, 2011. <br />
<a 
href="http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf">PDF</a> / <a 
href="http://scikit-learn.org">Homepage</a>
</li>

<li>API design for machine learning software: experiences from the scikit-learn project <br />
Lars Buitinck, Gilles Louppe, <u>Mathieu Blondel</u>, Fabian Pedregosa, Andreas
Müller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort,
Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly, Brian Holt, 
Gaël Varoquaux. <br />
<strong>ECML PKDD Workshop</strong>:
Languages for Data Mining and Machine Learning, September 2013. <br />
<a href="lbuitinck-ecmlpkdd2013.pdf">PDF</a>
</li>
</ul>

</div>

<div id="footer">
Copyright 2010-Present Mathieu Blondel
</div>

</body>
</html>
