<!DOCTYPE HTML PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Mathieu Blondel // Publications</title>
<link rel="stylesheet" href="../style.css" type="text/css" media="screen">
</head>

<body>

<div id="topbar">
  <div id="header">
    <h1>Mathieu Blondel</h1>
  </div>
</div>

<div id="navigation">
  <p>
  <a href="../">Home</a> /
  <a href="." class="active">Publications</a> /
  <a href="../talks/">Talks</a> /
  <a href="../teaching/">Teaching</a> /
  <a href="../students/">Students</a> /
  <a href="../code/">Software</a> /
  <a href="../data/">Data</a> 
  </p>
</div>

<div id="main">

<p>For a full list, see my Google scholar <a href="http://scholar.google.com/citations?user=C0EKzrUAAAAJ">profile</a>.</p>

<!--<h2>Preprints</h2>-->

<!--<ul class="pub">-->
<!--</ul>-->

<h2>Selected publications</h2>

<ul class="pub">

<li>
Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective.<br />
Michael Sander, Joan Puigcerver, Josip Djolonga, Gabriel Peyré, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2023. <br />
<a href="https://arxiv.org/abs/2302.01425">arXiv</a> /
<a href="https://github.com/google-research/google-research/tree/master/sparse_soft_topk">Code</a> /
<a href="../talks/mblondel-ICML-2023-07.pdf">Slides</a>
</li>

<li>
Sparsity-Constrained Optimal Transport.<br />
Tianlin Liu, Joan Puigcerver, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Learning Representations (<strong>ICLR</strong>), May 2023. <br />
<a href="https://arxiv.org/abs/2209.15466">arXiv</a>
</li>

<li>
Deep embedding and alignment of protein sequences.<br />
Felipe Llinares-López, Quentin Berthet, <u>Mathieu Blondel</u>,
Olivier Teboul, Jean-Philippe Vert. <br />
<strong>Nature Methods</strong>, volume 20, pages 104–111, 2023. <br />
<a href="https://www.nature.com/articles/s41592-022-01700-2">Link</a> /
<a href="https://www.biorxiv.org/content/10.1101/2021.11.15.468653">bioRxiv</a>
/ <a href="https://github.com/google-research/google-research/tree/master/dedal">Code</a>
</li>

<li>
Learning Energy Networks with Generalized Fenchel-Young Losses.<br />
<u>Mathieu Blondel</u>, Felipe Llinares-López, Robert Dadashi, Léonard Hussenot, Matthieu Geist. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2022. <br />
<a href="https://arxiv.org/abs/2205.09589">arXiv</a>
</li>

<li>
Efficient and Modular Implicit Differentiation. <br />
<u>Mathieu Blondel</u>, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-López, Fabian Pedregosa, Jean-Philippe Vert. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2022. <br />
<a href="https://arxiv.org/abs/2105.15183">arXiv</a> /
<a href="https://github.com/google/jaxopt">Code</a>
</li>

<li>
Sparse Continuous Distributions and Fenchel-Young Losses. <br />
André F. T. Martins, Marcos Treviso, António Farinhas, Pedro M. Q. Aguiar, Mário A. T. Figueiredo, <u>Mathieu Blondel</u>, Vlad Niculae. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>). <br />
<a href="https://arxiv.org/abs/2108.01988">arXiv</a> /
<a href="https://github.com/deep-spin/sparse_continuous_distributions">Code</a>
</li>

<li>
Implicit Differentiation for Fast Hyperparameter Selection in Non-Smooth Convex Learning. <br />
Quentin Bertrand, Quentin Klopfenstein, Mathurin Massias, <u>Mathieu Blondel</u>,
Samuel Vaiter, Alexandre Gramfort, Joseph Salmon. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>). <br />
<a href="https://arxiv.org/abs/2105.01637">arXiv</a> /
<a href="https://github.com/QB3/sparse-ho">Code</a>
</li>

<li>
Sinkformers: Transformers with Doubly Stochastic Attention.<br />
Michaël Sander, Pierre Ablin, <u>Mathieu Blondel</u>, Gabriel Peyré. <br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), March 2022. <br />
<a href="https://arxiv.org/abs/2110.11773">arXiv</a> / 
<a href="https://github.com/michaelsdr/sinkformers">Code</a>
</li>

<li>
Momentum Residual Neural Networks. <br />
Michaël Sander, Pierre Ablin, <u>Mathieu Blondel</u>, Gabriel Peyré. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2021. <br />
<a href="https://arxiv.org/abs/2102.07870">arXiv</a> /
<a href="https://github.com/michaelsdr/momentumnet">Code</a>
</li>

<li>
Differentiable Divergences Between Time Series. <br />
<u>Mathieu Blondel</u>, Arthur Mensch, Jean-Philippe Vert. <br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), April 2021. <br />
<a href="https://arxiv.org/abs/2010.08354">arXiv</a> /
<a href="https://github.com/google-research/soft-dtw-divergences">Code</a>
</li>

<li>
Learning with Differentiable Perturbed Optimizers. <br />
Quentin Berthet, <u>Mathieu Blondel</u>, Olivier Teboul, Marco Cuturi, Jean-Philippe Vert, Francis Bach. <br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2020. <br />
<a href="https://arxiv.org/abs/2002.08676">arXiv</a> / 
<a href="https://github.com/google-research/google-research/tree/master/perturbations">Code</a>
</li>

<li>
Fast Differentiable Sorting and Ranking. <br />
<u>Mathieu Blondel</u>, Olivier Teboul, Quentin Berthet, Josip Djolonga. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2020. <br />
<a href="https://arxiv.org/abs/2002.08871">arXiv</a> /
<a href="https://github.com/google-research/fast-soft-sort">Code</a> (see also PyTorch <a href="https://github.com/teddykoker/torchsort">reimplementation</a> by Teddy Koker).
</li>

<li>
Implicit Differentiation of Lasso-type Models for Hyperparameter Optimization. <br />
Quentin Bertrand, Quentin Klopfenstein, <u>Mathieu Blondel</u>, Samuel Vaiter, Alexandre Gramfort, Joseph Salmon. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2020. <br />
<a href="https://arxiv.org/abs/2002.08943">arXiv</a> /
<a href="https://github.com/QB3/sparse-ho">Code</a>
</li>

<li>
Structured Prediction with Projection Oracles. <br />
Mathieu Blondel.<br />
In Proceedings of Neural Information Processing Systems (<strong>NeurIPS</strong>), December 2019. <br />
<a href="https://arxiv.org/abs/1910.11369">arXiv</a> / <a href="https://github.com/mblondel/projection-losses">Code</a>
</li>

<li>
Geometric Losses for Distributional Learning. <br />
Arthur Mensch, <u>Mathieu Blondel</u>, Gabriel Peyré. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), June 2019. <br />
<a href="https://arxiv.org/abs/1905.06005">arXiv</a> / 
<a href="https://github.com/arthurmensch/g-softmax">Code</a>
</li>

<li>
Learning with Fenchel-Young Losses. <br />
<u>Mathieu Blondel</u>, André F. T. Martins, Vlad Niculae. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>), Volume 21, pp. 1−69, 2020. <br />
<a href="https://arxiv.org/abs/1901.02324">arXiv</a> /
<a href="https://github.com/mblondel/fenchel-young-losses">Code</a>
</li>

<li>
Learning Classifiers with Fenchel-Young Losses: Generalized Entropies, Margins, and Algorithms. <br />
<u>Mathieu Blondel</u>, André F. T. Martins, Vlad Niculae. <br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), April 2019. <br />
<a href="https://arxiv.org/abs/1805.09717">arXiv</a> /
<a href="https://github.com/mblondel/fenchel-young-losses">Code</a> 
</li>

<li>
SparseMAP: Differentiable Sparse Structured Inference. <br />
Vlad Niculae, André F. T. Martins, <u>Mathieu Blondel</u>, Claire Cardie. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2018. <br />
<a href="https://arxiv.org/abs/1802.04223">arXiv</a> / 
<a href="https://github.com/vene/sparsemap">Code</a>
</li>

<li>
Differentiable Dynamic Programming for Structured Prediction and Attention. <br />
Arthur Mensch, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), July 2018. <br />
<a href="https://arxiv.org/abs/1802.03676">arXiv</a> /
<a href="https://github.com/arthurmensch/didyprog">Code</a>
</li>

<li>
Large-Scale Optimal Transport and Mapping Estimation. <br />
Vivien Seguy, Bharath Bhushan Damodaran, Rémi Flamary, Nicolas Courty, Antoine Rolet, <u>Mathieu Blondel</u>. <br />
In Proceedings of International Conference on Learning Representations (<strong>ICLR</strong>), April 2018. <br />
<a href="https://arxiv.org/abs/1711.02283">arXiv</a> / 
<a href="https://github.com/vivienseguy/Large-Scale-OT">Code</a>
</li>

<li>
Smooth and Sparse Optimal Transport. <br />
<u>Mathieu Blondel</u>, Vivien Seguy, Antoine Rolet.<br />
In Proceedings of Artificial Intelligence and Statistics (<strong>AISTATS</strong>), April 2018. <br />
<a href="https://arxiv.org/abs/1710.06276">arXiv</a> / 
<a href="https://github.com/mblondel/smooth-ot">Code</a> /
<a href="https://github.com/mblondel/smooth-ot/tree/master/data">Data</a> 
</li>

<li>
A Regularized Framework for Sparse and Structured Neural Attention. <br />
Vlad Niculae, <u>Mathieu Blondel</u>.<br />
In Proceedings of Neural Information Processing Systems (<strong>NIPS</strong>), December 2017. <br />
<a href="https://arxiv.org/abs/1705.07704">arXiv</a> / 
<a href="https://github.com/vene/sparse-structured-attention">Code</a>
</li>

<li>
Multi-output Polynomial Networks and Factorization Machines. <br />
<u>Mathieu Blondel</u>, Vlad Niculae, Takuma Otsuka, Naonori Ueda.<br />
In Proceedings of Neural Information Processing Systems (<strong>NIPS</strong>), December 2017. <br />
<a href="https://arxiv.org/abs/1705.07603">arXiv</a> <br />
</li>

<li>
Soft-DTW: a Differentiable Loss Function for Time-Series. <br />
Marco Cuturi, <u>Mathieu Blondel</u>.<br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), August 2017. <br />
<a href="mcuturi-mblondel-icml2017.pdf">PDF</a> / 
<a href="https://github.com/mblondel/soft-dtw">Code</a>
</li>

<li>
Higher-order Factorization Machines. <br />
<u>Mathieu Blondel</u>, Akinori Fujino, Naonori Ueda, Masakazu Ishihata. <br />
In Proceedings of Neural Information Processing Systems (<strong>NIPS</strong>), December 2016. <br />
<a href="http://arxiv.org/abs/1607.07195">arXiv</a>
</li>

<li>
Polynomial Networks and Factorization Machines: New Insights and Efficient Training Algorithms. <br />
<u>Mathieu Blondel</u>, Masakazu Ishihata, Akinori Fujino, Naonori Ueda. <br />
In Proceedings of International Conference on Machine Learning (<strong>ICML</strong>), September 2016. <br />
<a href="mblondel-icml2016.pdf">PDF</a> <br />
Open-source <a href="https://github.com/scikit-learn-contrib/polylearn">implementation</a> by <a href="http://vene.ro/">Vlad Niculae</a>
</li>

<li>
Convex Factorization Machines. <br />
<u>Mathieu Blondel</u>, Akinori Fujino, Naonori Ueda. <br />
In Proceedings of European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (<strong>ECML PKDD</strong>), September 2015. <br />
<a href="mblondel-ecmlpkdd2015.pdf">PDF</a> / <a href="../talks/mblondel-cambridge-2015-09.pdf">Slides</a> <br />
Open-source <a href="https://gist.github.com/vene/d0fa89c17de961a18f7b">implementation</a> by <a href="http://vene.ro/">Vlad Niculae</a>
</li>

<li>
Large-scale Multiclass Support Vector Machine Training via Euclidean Projection onto the Simplex. <br />
<u>Mathieu Blondel</u>, Akinori Fujino, Naonori Ueda. <br />
In Proceedings of the 22nd International Conference on Pattern Recognition (<strong>ICPR</strong>), August 2014.<br />
<a href="mblondel-icpr2014.pdf">PDF</a> /
<a href="https://gist.github.com/mblondel/97cffbea574a5890f0d7">Code</a>
</li>

<li>
Online Passive-Aggressive Algorithms for Non-Negative Matrix Factorization and Completion. <br />
<u>Mathieu Blondel</u>, Yotaro Kubo, Naonori Ueda.  <br /> 
In Proceedings of the 17th International Conference on Artifical Intelligence and Statistics (<strong>AISTATS</strong>), April 2014. <br />
<a href="mblondel-aistats2014.pdf">PDF</a> / <a href="mblondel-aistats2014-supp.pdf">Supplementary material</a> 

<li>Block Coordinate Descent Algorithms for Large-scale Sparse Multiclass 
Classification. <br />
<u>Mathieu Blondel</u>, Kazuhiro Seki, Kuniaki Uehara. <br />
<strong>Machine Learning</strong>, May 2013. The final publication is available <a href="http://link.springer.com/article/10.1007%2Fs10994-013-5367-2">here</a>.<br/>
Open-science honorable mention at ECML PKDD 2013. <br />
<a href="mblondel-mlj2013.pdf">PDF</a> / <a href="https://github.com/scikit-learn-contrib/lightning">Code</a> / <a href="../data/">Data</a> 
</li>

</ul>

<h2>Machine learning software</h2>

<ul class="pub">
<li>Scikit-learn: Machine Learning in Python. <br />
Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand 
Thirion, Olivier Grisel, <u>Mathieu Blondel</u>, Peter Prettenhofer, Ron Weiss, 
Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu 
Brucher, Matthieu Perrot, Édouard Duchesnay. <br />
Journal of Machine Learning Research (<strong>JMLR</strong>), volume 12, pp. 2825−2830, 2011. <br />
<a 
href="http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf">PDF</a> / <a 
href="http://scikit-learn.org">Homepage</a>
</li>

<li>API design for machine learning software: experiences from the scikit-learn project <br />
Lars Buitinck, Gilles Louppe, <u>Mathieu Blondel</u>, Fabian Pedregosa, Andreas
Müller, Olivier Grisel, Vlad Niculae, Peter Prettenhofer, Alexandre Gramfort,
Jaques Grobler, Robert Layton, Jake Vanderplas, Arnaud Joly, Brian Holt, 
Gaël Varoquaux. <br />
<strong>ECML PKDD Workshop</strong>:
Languages for Data Mining and Machine Learning, September 2013. <br />
<a href="lbuitinck-ecmlpkdd2013.pdf">PDF</a>
</li>
</ul>

</div>

<div id="footer">
Copyright 2010-Present Mathieu Blondel
</div>

</body>
</html>
